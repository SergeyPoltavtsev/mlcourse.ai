{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ShuffleSplit, KFold\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../input'\n",
    "df_train_features = pd.read_csv(os.path.join(PATH_TO_DATA,'train_features.csv'), index_col='match_id_hash')\n",
    "df_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA,'train_targets.csv'), index_col='match_id_hash')\n",
    "df_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'), index_col='match_id_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train feature shape {0}\".format(df_train_features.shape))\n",
    "print(\"train target shape {0}\".format(df_train_targets.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_targets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_features.values\n",
    "y = df_train_targets['radiant_win'].values\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=17)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_score(model, X=X_train, y=y_train, X_val=X_valid, y_val=y_valid):\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "    \n",
    "    valid_score = roc_auc_score(y_valid, y_pred)\n",
    "    print('Validation ROC-AUC score:', valid_score)\n",
    "\n",
    "    valid_accuracy = accuracy_score(y_valid, y_pred > 0.5)\n",
    "    print('Validation accuracy of P>0.5 classifier:', valid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(df_submission):\n",
    "    submission_filename = 'submission_{}.csv'.format(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    df_submission.to_csv(submission_filename)\n",
    "    print('Submission saved to {}'.format(submission_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_write_submission_file(model):\n",
    "    X_test = df_test_features.values\n",
    "    y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    Y_pred_submission = pd.DataFrame({'radiant_win_prob': y_test_pred}, index=df_test_features.index)\n",
    "    \n",
    "    write_submission_file(Y_pred_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_write_submission_file(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 splits with 70%/30%\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=17)\n",
    "def get_cv_scores(model, X=X, y=y, cv=cv, scoring='roc_auc'):\n",
    "    return cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model_rf1 = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=None, random_state=17)\n",
    "\n",
    "# # calcuate ROC-AUC for each split\n",
    "# cv_scores_rf1 = cross_val_score(model_rf1, X, y, cv=cv, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_rf2 = RandomForestClassifier(n_estimators=100, n_jobs=-1, min_samples_leaf=3, random_state=17)\n",
    "\n",
    "cv_scores_rf2 = get_cv_scores(model_rf2)\n",
    "cv_scores_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 2 mean score:', cv_scores_rf2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_score(model_rf2)\n",
    "predict_and_write_submission_file(model_rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with all available information on Dota games\n",
    "> Raw data descriptions for all games are given in files `train_matches.jsonl` and `test_matches.jsonl`. Each file has one entry for each game in [JSON](https://en.wikipedia.org/wiki/JSON) format. You only need to know that it can be easily converted to Python objects via the `json.loads` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')) as fin:\n",
    "    # read the 18-th line\n",
    "    for i in range(18):\n",
    "        line = fin.readline()\n",
    "    \n",
    "    # read JSON into a Python object \n",
    "    match = json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = match['players'][2]\n",
    "#player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player['kills'], player['deaths'], player['assists']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player['ability_uses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for player in match['players']:\n",
    "    plt.plot(player['times'], player['gold_t'])\n",
    "    \n",
    "plt.title('Gold change for all players');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ujson\n",
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import ujson as json\n",
    "except ModuleNotFoundError:\n",
    "    import json\n",
    "    print ('Please install ujson to read JSON oblects faster')\n",
    "    \n",
    "try:\n",
    "    from tqdm import tqdm_notebook\n",
    "except ModuleNotFoundError:\n",
    "    tqdm_notebook = lambda x: x\n",
    "    print ('Please install tqdm to track progress with Python loops')\n",
    "\n",
    "def read_matches(matches_file):\n",
    "    \n",
    "    MATCHES_COUNT = {\n",
    "        'test_matches.jsonl': 10000,\n",
    "        'train_matches.jsonl': 39675,\n",
    "    }\n",
    "    _, filename = os.path.split(matches_file)\n",
    "    total_matches = MATCHES_COUNT.get(filename)\n",
    "    \n",
    "    with open(matches_file) as fin:\n",
    "        for line in tqdm_notebook(fin, total=total_matches):\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for match in read_matches(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')):\n",
    "#     match_id_hash = match['match_id_hash']\n",
    "#     game_time = match['game_time']\n",
    "    \n",
    "#     # processing each game\n",
    "    \n",
    "#     for player in match['players']:\n",
    "#         pass  # processing each player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(df_features, matches_file):\n",
    "    \n",
    "    # Process raw data and add new features\n",
    "    for match in read_matches(matches_file):\n",
    "        match_id_hash = match['match_id_hash']\n",
    "\n",
    "        # Counting ruined towers for both teams\n",
    "        radiant_tower_kills = 0\n",
    "        dire_tower_kills = 0\n",
    "        for objective in match['objectives']:\n",
    "            if objective['type'] == 'CHAT_MESSAGE_TOWER_KILL':\n",
    "                if objective['team'] == 2:\n",
    "                    radiant_tower_kills += 1\n",
    "                if objective['team'] == 3:\n",
    "                    dire_tower_kills += 1\n",
    "\n",
    "        # Write new features\n",
    "        df_features.loc[match_id_hash, 'radiant_tower_kills'] = radiant_tower_kills\n",
    "        df_features.loc[match_id_hash, 'dire_tower_kills'] = dire_tower_kills\n",
    "        df_features.loc[match_id_hash, 'diff_tower_kills'] = radiant_tower_kills - dire_tower_kills\n",
    "        \n",
    "        # ... here you can add more features ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataframe with features\n",
    "df_train_features_extended = df_train_features.copy()\n",
    "\n",
    "# add new features\n",
    "add_new_features(df_train_features_extended, \n",
    "                 os.path.join(PATH_TO_DATA, \n",
    "                              'train_matches.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features_extended.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=4, min_samples_leaf=3, random_state=17)\n",
    "\n",
    "cv_scores_base = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "cv_scores_extended = cross_val_score(model, df_train_features_extended.values, y, \n",
    "                                     cv=cv, scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Base features: mean={} scores={}'.format(cv_scores_base.mean(), \n",
    "                                                cv_scores_base))\n",
    "print('Extended features: mean={} scores={}'.format(cv_scores_extended.mean(), \n",
    "                                                    cv_scores_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_extended > cv_scores_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build the same features for the test set\n",
    "df_test_features_extended = df_test_features.copy()\n",
    "add_new_features(df_test_features_extended, \n",
    "                 os.path.join(PATH_TO_DATA, 'test_matches.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=17)\n",
    "model.fit(X, y)\n",
    "df_submission_base = pd.DataFrame(\n",
    "    {'radiant_win_prob': model.predict_proba(df_test_features.values)[:, 1]}, \n",
    "    index=df_test_features.index,\n",
    ")\n",
    "df_submission_base.to_csv('submission_base_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extended = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=17)\n",
    "model_extended.fit(df_train_features_extended.values, y)\n",
    "df_submission_extended = pd.DataFrame(\n",
    "    {'radiant_win_prob': model_extended.predict_proba(df_test_features_extended.values)[:, 1]}, \n",
    "    index=df_test_features.index,\n",
    ")\n",
    "df_submission_extended.to_csv('submission_extended_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one will be used as a final submission in this kernel\n",
    "!cp submission_extended_rf.csv submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
